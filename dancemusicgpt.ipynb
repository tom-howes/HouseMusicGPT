{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 14198350,
          "sourceType": "datasetVersion",
          "datasetId": 9054634
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tom-howes/DanceMusicGPT/blob/main/dancemusicgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install miditok>=3.0.0 symusic torch tqdm matplotlib"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:14.822511Z",
          "iopub.execute_input": "2025-12-17T19:03:14.822765Z",
          "iopub.status.idle": "2025-12-17T19:03:21.388680Z",
          "shell.execute_reply.started": "2025-12-17T19:03:14.822734Z",
          "shell.execute_reply": "2025-12-17T19:03:21.387706Z"
        },
        "id": "TKj-RrmX3cIl"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dance-midi\n",
        "!mv *.mid dance-midi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29vl5Sfl36eS",
        "outputId": "65abf2d7-f703-4782-b72d-56581c88e93b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dance-midi’: File exists\n",
            "mv: cannot stat '*.mid': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from symusic import Score\n",
        "\n",
        "midi_files = list(Path('dance-midi').glob('*.mid')) + list(Path('dance-midi').glob('*.midi'))\n",
        "print(f\"Found {len(midi_files)} MIDI files\\n\")\n",
        "## Optional Print Statements for track analysis ##\n",
        "# Analyze a few\n",
        "# total_bpm = 0\n",
        "# for midi_path in midi_files:\n",
        "#     try:\n",
        "#         score = Score(str(midi_path))\n",
        "#         print(f\"{midi_path.name}\")\n",
        "#         print(f\"  Duration: {score.end() / score.ticks_per_quarter / 4:.1f} bars (assuming 4/4) BPM: \", score.tempos[0].qpm)\n",
        "#         print(f\"  Tracks: {len(score.tracks)}\")\n",
        "#         for track in score.tracks:\n",
        "#             print(f\"    - {track.name or 'Unnamed'}: {len(track.notes)} notes, program {track.program}\")\n",
        "#         print()\n",
        "#         if score.tempos[0].qpm < 50 or score.tempos[0].qpm > 150:\n",
        "#           total_bpm += 120\n",
        "#         else:\n",
        "#           total_bpm += score.tempos[0].qpm\n",
        "#     except Exception as e:\n",
        "#         print(f\"{midi_path.name}: Error - {e}\\n\")\n",
        "\n",
        "\n",
        "# print(total_bpm / 38)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:21.390750Z",
          "iopub.execute_input": "2025-12-17T19:03:21.391008Z",
          "iopub.status.idle": "2025-12-17T19:03:22.033447Z",
          "shell.execute_reply.started": "2025-12-17T19:03:21.390979Z",
          "shell.execute_reply": "2025-12-17T19:03:22.032871Z"
        },
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVPJGIdq3cIm",
        "outputId": "ca37810d-66dc-4b94-d7c7-d8190f09191e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 38 MIDI files\n",
            "\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "from miditok import REMI, TokenizerConfig\n",
        "\n",
        "# Configure tokenizer for dance music\n",
        "config = TokenizerConfig(\n",
        "    num_velocities=16,          # Quantize velocity into 16 bins\n",
        "    use_chords=False,                    # Enable chord detection\n",
        "    # chord_tokens_with_root_note=True,    # Include root note (e.g. \"Chord_C:maj\")\n",
        "    use_programs=True,                  # Enable multi-instrument\n",
        "    use_time_signatures=True,\n",
        "    use_tempos=True,                    # Allows model to predict changes in tempo\n",
        "    num_tempos=32,                      # Number of tempo bins\n",
        "    tempo_range=(100, 140),             # Dance music tempo range\n",
        "    one_token_stream_for_programs=True,  # Prepend instrument to Pitch, NoteOn, NoteOff (test with initially - might set to false to generate individual instruments if I run into issues)\n",
        "    beat_res={(0, 4): 8, (4, 12): 4},\n",
        ")\n",
        "\n",
        "tokenizer = REMI(config)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:22.034234Z",
          "iopub.execute_input": "2025-12-17T19:03:22.034494Z",
          "iopub.status.idle": "2025-12-17T19:03:26.245807Z",
          "shell.execute_reply.started": "2025-12-17T19:03:22.034472Z",
          "shell.execute_reply": "2025-12-17T19:03:26.245078Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBkA75-R3cIo",
        "outputId": "448f1cad-7d2f-4c52-e9b7-8aa1f1f67f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
            "  super().__init__(tokenizer_config, params)\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = []\n",
        "total_tokens = 0\n",
        "### Optional print for token count ###\n",
        "# print(f\"{'File':<60} {'Tokens':<10}\")\n",
        "# print(\"-\" * 70)\n",
        "\n",
        "for midi_path in midi_files:\n",
        "  try:\n",
        "    tokens = tokenizer(midi_path)\n",
        "    all_tokens.append(tokens)\n",
        "    total_tokens += len(tokens.ids)\n",
        "    # print(f\"{midi_path.name:<60} {len(tokens.ids):<10}\")\n",
        "  except Exception as e:\n",
        "    print(f\"{midi_path.name:<60} Error: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:26.246766Z",
          "iopub.execute_input": "2025-12-17T19:03:26.247300Z",
          "iopub.status.idle": "2025-12-17T19:03:29.765056Z",
          "shell.execute_reply.started": "2025-12-17T19:03:26.247275Z",
          "shell.execute_reply": "2025-12-17T19:03:29.764463Z"
        },
        "id": "qFoecB-U3cIp"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate -q"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:29.765953Z",
          "iopub.execute_input": "2025-12-17T19:03:29.766374Z",
          "iopub.status.idle": "2025-12-17T19:03:32.973004Z",
          "shell.execute_reply.started": "2025-12-17T19:03:29.766348Z",
          "shell.execute_reply": "2025-12-17T19:03:32.972218Z"
        },
        "id": "j-iwZsZp3cIp"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import torch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:32.974323Z",
          "iopub.execute_input": "2025-12-17T19:03:32.974646Z",
          "iopub.status.idle": "2025-12-17T19:03:57.869274Z",
          "shell.execute_reply.started": "2025-12-17T19:03:32.974609Z",
          "shell.execute_reply": "2025-12-17T19:03:57.868679Z"
        },
        "id": "4ZA-AwVu3cIp"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for tokens\n",
        "class MidiDataset(Dataset):\n",
        "  def __init__(self, all_tokens, seq_length=512, stride=512):\n",
        "    self.seq_length = seq_length\n",
        "\n",
        "    self.data = []\n",
        "    for token in all_tokens:\n",
        "      self.data.extend(tokens)\n",
        "\n",
        "    self.data = torch.tensor(self.data, dtype=torch.long)\n",
        "    # Use stride to avoid non-overlapping chunks for faster training\n",
        "    self.indices = list(range(0, len(self.data) - seq_length - 1, stride))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    start = self.indices[idx]\n",
        "    chunk = self.data[start:start + self.seq_length + 1]\n",
        "    return {\n",
        "        'input_ids': chunk[:-1],\n",
        "        'labels' : chunk[1:]\n",
        "    }\n",
        "# Create dataset\n",
        "dataset = MidiDataset(all_tokens, seq_length=512, stride=256)\n",
        "print(f\"Dataset samples: {len(dataset)}\")\n",
        "print(f\"Steps per epoch: {len(dataset) // 8}\")\n",
        "print(f\"Total steps (10 epochs): {len(dataset) // 8 * 10}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:57.871062Z",
          "iopub.execute_input": "2025-12-17T19:03:57.871593Z",
          "iopub.status.idle": "2025-12-17T19:03:58.470118Z",
          "shell.execute_reply.started": "2025-12-17T19:03:57.871568Z",
          "shell.execute_reply": "2025-12-17T19:03:58.469477Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj_ytXA-3cIq",
        "outputId": "e2203c22-050c-47d7-be95-d9d8474a8b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset samples: 941\n",
            "Steps per epoch: 117\n",
            "Total steps (10 epochs): 1170\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config(\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_positions=512,\n",
        "    n_embd=256,\n",
        "    n_layer=6,\n",
        "    n_head=8,\n",
        "    bos_token_id=tokenizer['BOS_None'],\n",
        "    eos_token_id=tokenizer['EOS_None']\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)\n",
        "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:58.471086Z",
          "iopub.execute_input": "2025-12-17T19:03:58.471491Z",
          "iopub.status.idle": "2025-12-17T19:03:58.575415Z",
          "shell.execute_reply.started": "2025-12-17T19:03:58.471463Z",
          "shell.execute_reply": "2025-12-17T19:03:58.574690Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XINTM9i93cIq",
        "outputId": "7de96bd4-4e27-4d18-81ab-9830e205a0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Parameters: 4,998,656\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 1: 10 epochs, batch_size = 8"
      ],
      "metadata": {
        "id": "qS4NAqLg3cIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load tensorboard extension\n",
        "# %load_ext tensorboard\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./midi-gpt2\",\n",
        "#     overwrite_output_dir=True,\n",
        "#     num_train_epochs=10,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     learning_rate=5e-4,\n",
        "#     warmup_steps=100,\n",
        "#     logging_steps=50,\n",
        "#     logging_dir=\"./logs\",       # TensorBoard logs\n",
        "#     save_steps=500,\n",
        "#     save_total_limit=2,\n",
        "#     report_to=\"tensorboard\",    # Use tensorboard\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=dataset,\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:58.576319Z",
          "iopub.execute_input": "2025-12-17T19:03:58.576594Z",
          "iopub.status.idle": "2025-12-17T19:03:58.580015Z",
          "shell.execute_reply.started": "2025-12-17T19:03:58.576571Z",
          "shell.execute_reply": "2025-12-17T19:03:58.579376Z"
        },
        "id": "TZiURO133cIs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate a sequence\n",
        "# model.eval()\n",
        "\n",
        "# # Start with a bar token\n",
        "# start_tokens = torch.tensor([[tokenizer['Bar_None']]], device=device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     output = model.generate(\n",
        "#         start_tokens,\n",
        "#         max_length=500,\n",
        "#         temperature=1.0,\n",
        "#         top_p=0.95,\n",
        "#         do_sample=True,\n",
        "#     )\n",
        "\n",
        "# # Decode\n",
        "# generated = [tokenizer[tok_id.item()] for tok_id in output[0]]\n",
        "# print(\"Generated tokens:\")\n",
        "# for i, tok in enumerate(generated[:100]):\n",
        "#     print(f\"  {i}: {tok}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:58.580814Z",
          "iopub.execute_input": "2025-12-17T19:03:58.581120Z",
          "iopub.status.idle": "2025-12-17T19:03:58.596735Z",
          "shell.execute_reply.started": "2025-12-17T19:03:58.581098Z",
          "shell.execute_reply": "2025-12-17T19:03:58.596053Z"
        },
        "id": "3ugIAqKR3cIs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Decode the generated tokens back to MIDI\n",
        "# from miditok import TokSequence\n",
        "\n",
        "# # Get the generated token IDs (not strings)\n",
        "# generated_ids = output[0].tolist()\n",
        "\n",
        "# # Create a TokSequence and decode to MIDI\n",
        "# generated_midi = tokenizer.decode(generated_ids)\n",
        "# generated_midi.dump_midi(\"generated_sample.mid\")\n",
        "\n",
        "# print(\"Saved to generated_sample.mid\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:58.597524Z",
          "iopub.execute_input": "2025-12-17T19:03:58.597777Z",
          "iopub.status.idle": "2025-12-17T19:03:58.609791Z",
          "shell.execute_reply.started": "2025-12-17T19:03:58.597756Z",
          "shell.execute_reply": "2025-12-17T19:03:58.609171Z"
        },
        "id": "fwcU191l3cIs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import FileLink\n",
        "# FileLink(\"generated_sample.mid\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:58.610577Z",
          "iopub.execute_input": "2025-12-17T19:03:58.610759Z",
          "iopub.status.idle": "2025-12-17T19:03:58.622270Z",
          "shell.execute_reply.started": "2025-12-17T19:03:58.610740Z",
          "shell.execute_reply": "2025-12-17T19:03:58.621699Z"
        },
        "id": "-VX5Zbkj3cIs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for more epochs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./midi-gpt2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate=3e-4,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:03:58.623072Z",
          "iopub.execute_input": "2025-12-17T19:03:58.623402Z",
          "iopub.status.idle": "2025-12-17T19:41:25.700538Z",
          "shell.execute_reply.started": "2025-12-17T19:03:58.623370Z",
          "shell.execute_reply": "2025-12-17T19:41:25.699901Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AMdpOHjy3cIs",
        "outputId": "73241c25-0f8c-4700-9ac2-50b03f356908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2950' max='2950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2950/2950 04:16, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.418700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.354400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.858300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.506800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.289000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.160200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.096500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.060500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.042300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.034200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.027000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.022800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.017700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.014600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.012700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.012100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.011900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.011100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.011000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.010700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.010400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.010200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.009800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2950, training_loss=0.24092079044398615, metrics={'train_runtime': 258.3668, 'train_samples_per_second': 182.105, 'train_steps_per_second': 11.418, 'total_flos': 684974093107200.0, 'train_loss': 0.24092079044398615, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:46:48.545867Z",
          "iopub.execute_input": "2025-12-17T19:46:48.546565Z",
          "iopub.status.idle": "2025-12-17T19:46:48.552733Z",
          "shell.execute_reply.started": "2025-12-17T19:46:48.546534Z",
          "shell.execute_reply": "2025-12-17T19:46:48.551925Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "0QpVNA7J3cIt",
        "outputId": "7dde1af6-3ec6-4ea8-cb81-95bd304e3145"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4193714530.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_peak_memory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "device = 'cpu'\n",
        "model.to(device)\n",
        "# Seed with real music\n",
        "seed_length = 50\n",
        "seed_tokens = torch.tensor([all_tokens[0][:seed_length]], device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=2000,        # Longer output\n",
        "        temperature=0.9,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "generated_midi = tokenizer.decode(generated_ids)\n",
        "generated_midi.dump_midi(\"generated_v2.mid\")\n",
        "\n",
        "# Check\n",
        "score = Score(\"generated_v2.mid\")\n",
        "print(f\"Tracks: {len(score.tracks)}\")\n",
        "print(f\"Total notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "print(f\"Duration: {score.end() / score.ticks_per_quarter / 2:.1f} bars\")\n",
        "\n",
        "from IPython.display import FileLink\n",
        "FileLink(\"generated_v2.mid\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T19:42:16.654495Z",
          "iopub.execute_input": "2025-12-17T19:42:16.654831Z",
          "iopub.status.idle": "2025-12-17T19:42:16.667177Z",
          "shell.execute_reply.started": "2025-12-17T19:42:16.654804Z",
          "shell.execute_reply": "2025-12-17T19:42:16.666160Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "D35J2tz53cIt",
        "outputId": "013bb170-6443-4106-b75e-0b467949874c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1002914556.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Seed with real music\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mseed_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4341\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4342\u001b[0m                 )\n\u001b[0;32m-> 4343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     )\n\u001b[0;32m-> 1357\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the correct vocab size and special tokens\n",
        "vocab_size = len(tokenizer)\n",
        "pad_token_id = tokenizer['PAD_None']\n",
        "eos_token_id = tokenizer['EOS_None']\n",
        "bos_token_id = tokenizer['BOS_None']\n",
        "\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(f\"PAD: {pad_token_id}, EOS: {eos_token_id}, BOS: {bos_token_id}\")\n",
        "\n",
        "# Check seed tokens are valid\n",
        "seed_data = tokens.ids[:50]\n",
        "print(f\"Seed token range: {min(seed_data)} to {max(seed_data)}\")\n",
        "\n",
        "# Move model to CPU\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./midi-gpt2/checkpoint-2950\")\n",
        "model = model.to('cpu')\n",
        "model.eval()\n",
        "\n",
        "# Create seed on CPU (no device argument)\n",
        "seed_tokens = torch.tensor([all_tokens[0][:50]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.9,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "print(f\"Generated {len(generated_ids)} tokens\")\n",
        "\n",
        "generated_midi = tokenizer.decode(generated_ids)\n",
        "generated_midi.dump_midi(\"generated.mid\")\n",
        "\n",
        "from symusic import Score\n",
        "score = Score(\"generated.mid\")\n",
        "print(f\"Notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "5FgC8a7E8AuI",
        "outputId": "69caa67b-92d4-4629-a4e6-6978ea2ca495"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 502\n",
            "PAD: 0, EOS: 2, BOS: 1\n",
            "Seed token range: 4 to 501\n",
            "Generated 500 tokens\n",
            "Notes: 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a28ac39f-ce99-4765-9660-254e1f66a87b\", \"generated.mid\", 114)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with just a Bar token\n",
        "start_tokens = torch.tensor([[tokenizer['Bar_None']]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        start_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.95,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "print(f\"Generated {len(generated_ids)} tokens\")\n",
        "\n",
        "generated_midi = tokenizer.decode(generated_ids)\n",
        "generated_midi.dump_midi(\"generated_no_seed.mid\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_no_seed.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "9AxtOIeF9fO6",
        "outputId": "2a059afc-b2c7-4653-a400-d270c121a91a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 500 tokens\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_431a3182-fd70-4ea2-9f42-f18e6fc48dfe\", \"generated_no_seed.mid\", 41)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_long(model, tokenizer, num_chunks=4, chunk_length=450, overlap=50):\n",
        "    \"\"\"Generate longer sequences by chaining chunks together\"\"\"\n",
        "\n",
        "    # Start with a bar token\n",
        "    generated = [tokenizer['Bar_None']]\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        # Use last `overlap` tokens as context\n",
        "        context = generated[-overlap:] if len(generated) > overlap else generated\n",
        "        input_tokens = torch.tensor([context])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_tokens,\n",
        "                max_length=len(context) + chunk_length,\n",
        "                temperature=0.9,\n",
        "                top_p=0.95,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer['PAD_None'],\n",
        "                eos_token_id=tokenizer['EOS_None'],\n",
        "            )\n",
        "\n",
        "        # Add new tokens (skip the context we provided)\n",
        "        new_tokens = output[0].tolist()[len(context):]\n",
        "        generated.extend(new_tokens)\n",
        "        print(f\"Chunk {i+1}: {len(new_tokens)} new tokens, total: {len(generated)}\")\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Generate ~2000 tokens (roughly 16-32 bars)\n",
        "generated_ids = generate_long(model, tokenizer, num_chunks=4, chunk_length=450, overlap=50)\n",
        "\n",
        "generated_midi = tokenizer.decode(generated_ids)\n",
        "generated_midi.dump_midi(\"generated_long.mid\")\n",
        "\n",
        "from symusic import Score\n",
        "score = Score(\"generated_long.mid\")\n",
        "print(f\"Total notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "print(f\"Duration: ~{score.end() / score.ticks_per_quarter / 4:.0f} bars\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_long.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "WNX7FGWm-hRg",
        "outputId": "9c7527a8-3387-411f-f3a9-2e29a3fd23d3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: 450 new tokens, total: 451\n",
            "Chunk 2: 450 new tokens, total: 901\n",
            "Chunk 3: 450 new tokens, total: 1351\n",
            "Chunk 4: 450 new tokens, total: 1801\n",
            "Total notes: 0\n",
            "Duration: ~0 bars\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bf265a25-d72d-4822-bbff-93ed51cdbd3b\", \"generated_long.mid\", 41)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_tokens = torch.tensor([[tokenizer['Bar_None']]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        start_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.7,        # Lower = more conservative\n",
        "        top_k=50,               # Limit choices\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.2, # Penalize repetition\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "\n",
        "# Check output\n",
        "print(\"First 50 tokens:\")\n",
        "for i, tok_id in enumerate(generated_ids[:50]):\n",
        "    print(f\"  {i}: {tokenizer[tok_id]}\")\n",
        "\n",
        "generated_midi = tokenizer.decode(generated_ids)\n",
        "generated_midi.dump_midi(\"generated_v2.mid\")\n",
        "\n",
        "from symusic import Score\n",
        "score = Score(\"generated_v2.mid\")\n",
        "print(f\"\\nTotal notes: {sum(len(t.notes) for t in score.tracks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP4T7-mp-0e9",
        "outputId": "0a99df41-34a5-4006-ec62-eadc518a1ec4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 50 tokens:\n",
            "  0: Bar_None\n",
            "  1: Position_0\n",
            "  2: Pitch_29\n",
            "  3: Duration_0.5.8\n",
            "  4: Pitch_80\n",
            "  5: Duration_0.3.8\n",
            "  6: Pitch_68\n",
            "  7: Duration_0.3.8\n",
            "  8: Pitch_65\n",
            "  9: Duration_0.3.8\n",
            "  10: Pitch_72\n",
            "  11: Duration_0.3.8\n",
            "  12: PitchDrum_35\n",
            "  13: Duration_0.1.8\n",
            "  14: Pitch_72\n",
            "  15: Duration_0.3.8\n",
            "  16: Pitch_73\n",
            "  17: Duration_0.3.8\n",
            "  18: Pitch_70\n",
            "  19: Duration_0.3.8\n",
            "  20: Pitch_65\n",
            "  21: Duration_0.3.8\n",
            "  22: Pitch_61\n",
            "  23: Duration_0.3.8\n",
            "  24: Pitch_65\n",
            "  25: Duration_0.3.8\n",
            "  26: Pitch_61\n",
            "  27: Duration_0.3.8\n",
            "  28: Pitch_65\n",
            "  29: Duration_4.0.4\n",
            "  30: Pitch_58\n",
            "  31: Duration_4.0.4\n",
            "  32: Pitch_54\n",
            "  33: Duration_4.0.4\n",
            "  34: PitchDrum_35\n",
            "  35: Duration_0.1.8\n",
            "  36: Program_76\n",
            "  37: Velocity_111\n",
            "  38: Program_-1\n",
            "  39: Velocity_127\n",
            "  40: Program_99\n",
            "  41: Velocity_111\n",
            "  42: Program_99\n",
            "  43: Velocity_111\n",
            "  44: Program_0\n",
            "  45: Velocity_7\n",
            "  46: Program_0\n",
            "  47: Velocity_7\n",
            "  48: Program_0\n",
            "  49: Velocity_7\n",
            "\n",
            "Total notes: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use first 20 tokens from real data as seed\n",
        "seed = all_tokens[0][:20]\n",
        "print(\"Seed tokens:\")\n",
        "for i, tok_id in enumerate(seed):\n",
        "    print(f\"  {i}: {tokenizer[tok_id]}\")\n",
        "\n",
        "seed_tokens = torch.tensor([seed])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "generated_midi = tokenizer.decode(generated_ids)\n",
        "generated_midi.dump_midi(\"generated_seeded.mid\")\n",
        "\n",
        "from symusic import Score\n",
        "score = Score(\"generated_seeded.mid\")\n",
        "print(f\"Total notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_seeded.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "RJid-pQ-_NUZ",
        "outputId": "69e01ba6-fadc-4487-f55e-733034628a9d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed tokens:\n",
            "  0: Bar_None\n",
            "  1: TimeSig_4/4\n",
            "  2: Position_0\n",
            "  3: Tempo_140.0\n",
            "  4: Bar_None\n",
            "  5: TimeSig_4/4\n",
            "  6: Bar_None\n",
            "  7: TimeSig_4/4\n",
            "  8: Position_16\n",
            "  9: Program_-1\n",
            "  10: PitchDrum_36\n",
            "  11: Velocity_79\n",
            "  12: Duration_1.0.8\n",
            "  13: Position_24\n",
            "  14: Program_-1\n",
            "  15: PitchDrum_38\n",
            "  16: Velocity_103\n",
            "  17: Duration_0.2.8\n",
            "  18: Program_-1\n",
            "  19: PitchDrum_36\n",
            "Total notes: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e6f20073-fe1b-465a-a79a-a83486c98c3e\", \"generated_seeded.mid\", 70)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Longer Seed more instruments"
      ],
      "metadata": {
        "id": "5WwDWKhg_qdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use first 100 tokens from a full song as seed\n",
        "seed = all_tokens[0][:100]\n",
        "\n",
        "seed_tokens = torch.tensor([seed])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.85,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "generated_midi = tokenizer.decode(generated_ids)\n",
        "generated_midi.dump_midi(\"generated_v3.mid\")\n",
        "\n",
        "from symusic import Score\n",
        "score = Score(\"generated_v3.mid\")\n",
        "print(f\"Total notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "print(f\"Duration: ~{score.end() / score.ticks_per_quarter / 4:.0f} bars\")\n",
        "\n",
        "# Check what instruments\n",
        "for track in score.tracks:\n",
        "    if len(track.notes) > 0:\n",
        "        print(f\"  Program {track.program}: {len(track.notes)} notes\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_v3.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "IDgvuzFB_e5E",
        "outputId": "6d2dfee5-f433-473e-d766-a0807dac32ae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total notes: 18\n",
            "Duration: ~6 bars\n",
            "  Program 0: 18 notes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f44fcf4c-73cf-4032-a130-bf4a014f25a9\", \"generated_v3.mid\", 186)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring different songs as seeds"
      ],
      "metadata": {
        "id": "P3nYThXd_2_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try seeds from different songs\n",
        "for i, song_tokens in enumerate(all_tokens[:5]):\n",
        "    seed = song_tokens[:100]\n",
        "    seed_tokens = torch.tensor([seed])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            seed_tokens,\n",
        "            max_length=500,\n",
        "            temperature=0.85,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer['PAD_None'],\n",
        "            eos_token_id=tokenizer['EOS_None'],\n",
        "        )\n",
        "\n",
        "    generated_ids = output[0].tolist()\n",
        "    score = tokenizer.decode(generated_ids)  # This returns a Score directly\n",
        "\n",
        "    total_notes = sum(len(t.notes) for t in score.tracks)\n",
        "    print(f\"Song {i} seed: {total_notes} notes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOfR3bdb_0_D",
        "outputId": "5c30bc77-eb7f-403b-b391-78fea3d36a0b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Song 0 seed: 18 notes\n",
            "Song 1 seed: 20 notes\n",
            "Song 2 seed: 16 notes\n",
            "Song 3 seed: 21 notes\n",
            "Song 4 seed: 18 notes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issue with drums only - Exploring using middle of track as seed"
      ],
      "metadata": {
        "id": "197_yiKiAeXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed from middle of songs (skip intro)\n",
        "for i, song_tokens in enumerate(all_tokens[:5]):\n",
        "    # Skip first 500 tokens (intro), take 100 from the middle\n",
        "    start_pos = min(500, len(song_tokens) // 2)\n",
        "    seed = song_tokens[start_pos:start_pos + 100]\n",
        "    seed_tokens = torch.tensor([seed])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            seed_tokens,\n",
        "            max_length=500,\n",
        "            temperature=0.85,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer['PAD_None'],\n",
        "            eos_token_id=tokenizer['EOS_None'],\n",
        "        )\n",
        "\n",
        "    generated_ids = output[0].tolist()\n",
        "    score = tokenizer.decode(generated_ids)\n",
        "\n",
        "    total_notes = sum(len(t.notes) for t in score.tracks)\n",
        "\n",
        "    # Count by instrument type\n",
        "    drum_notes = sum(len(t.notes) for t in score.tracks if t.program == -1 or t.is_drum)\n",
        "    other_notes = total_notes - drum_notes\n",
        "\n",
        "    print(f\"Song {i}: {total_notes} notes ({drum_notes} drums, {other_notes} other)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT9FHdveAZ_X",
        "outputId": "6675c9f4-1a3c-41ea-91b6-76bfce90053e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Song 0: 22 notes (6 drums, 16 other)\n",
            "Song 1: 21 notes (7 drums, 14 other)\n",
            "Song 2: 21 notes (2 drums, 19 other)\n",
            "Song 3: 22 notes (13 drums, 9 other)\n",
            "Song 4: 22 notes (3 drums, 19 other)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use middle of song 2 as seed (good drum/other ratio)\n",
        "song_tokens = all_tokens[2]\n",
        "start_pos = len(song_tokens) // 2\n",
        "seed = song_tokens[start_pos:start_pos + 100]\n",
        "\n",
        "seed_tokens = torch.tensor([seed])\n",
        "\n",
        "# Generate longer with chunking\n",
        "def generate_long(model, tokenizer, seed, num_chunks=6, chunk_length=400, overlap=50):\n",
        "    generated = list(seed)\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        context = generated[-overlap:]\n",
        "        input_tokens = torch.tensor([context])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_tokens,\n",
        "                max_length=len(context) + chunk_length,\n",
        "                temperature=0.85,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.2,\n",
        "                pad_token_id=tokenizer['PAD_None'],\n",
        "                eos_token_id=tokenizer['EOS_None'],\n",
        "            )\n",
        "\n",
        "        new_tokens = output[0].tolist()[len(context):]\n",
        "        generated.extend(new_tokens)\n",
        "\n",
        "        # Progress check\n",
        "        score = tokenizer.decode(generated)\n",
        "        notes = sum(len(t.notes) for t in score.tracks)\n",
        "        print(f\"Chunk {i+1}: {notes} total notes\")\n",
        "\n",
        "    return generated\n",
        "\n",
        "generated_ids = generate_long(model, tokenizer, seed, num_chunks=6)\n",
        "\n",
        "# Save and download\n",
        "score = tokenizer.decode(generated_ids)\n",
        "score.dump_midi(\"generated_full.mid\")\n",
        "\n",
        "total_notes = sum(len(t.notes) for t in score.tracks)\n",
        "drum_notes = sum(len(t.notes) for t in score.tracks if t.is_drum)\n",
        "print(f\"\\nFinal: {total_notes} notes ({drum_notes} drums, {total_notes - drum_notes} other)\")\n",
        "print(f\"Duration: ~{score.end() / score.ticks_per_quarter / 4:.0f} bars\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_full.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "EsfxSB5WAxMJ",
        "outputId": "22fc654f-09d0-4976-be31-24a8e30f0a95"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: 23 total notes\n",
            "Chunk 2: 23 total notes\n",
            "Chunk 3: 23 total notes\n",
            "Chunk 4: 23 total notes\n",
            "Chunk 5: 23 total notes\n",
            "Chunk 6: 23 total notes\n",
            "\n",
            "Final: 23 notes (2 drums, 21 other)\n",
            "Duration: ~2 bars\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_78aadda4-4946-4f75-9187-0f878bbbe914\", \"generated_full.mid\", 270)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lower temperature, top 20 choices, higher repetition penalty"
      ],
      "metadata": {
        "id": "Pl-B3oqvBsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "song_tokens = all_tokens[2]\n",
        "start_pos = len(song_tokens) // 2\n",
        "seed = song_tokens[start_pos:start_pos + 100]\n",
        "\n",
        "seed_tokens = torch.tensor([seed])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.5,          # Much lower - more predictable\n",
        "        top_k=20,                 # Only top 20 choices\n",
        "        top_p=0.85,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.3,   # Stronger repetition penalty\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "score = tokenizer.decode(generated_ids)\n",
        "score.dump_midi(\"generated_conservative.mid\")\n",
        "\n",
        "print(f\"Notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_conservative.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "HiY9HD1dBPSC",
        "outputId": "37747644-5773-460f-8d40-90655be1acc6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notes: 23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_17774f99-896c-44e4-a731-85404abf72a8\", \"generated_conservative.mid\", 270)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy gen"
      ],
      "metadata": {
        "id": "SHFeiylNBydb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=500,\n",
        "        do_sample=False,          # Greedy - always pick most likely\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "score = tokenizer.decode(generated_ids)\n",
        "score.dump_midi(\"generated_greedy.mid\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_greedy.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dTln0IDUBWjk",
        "outputId": "ae437bc2-e2ea-4829-ce51-9b9c24ac2798"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0b37b13c-37bd-4813-a52d-46ff9e693889\", \"generated_greedy.mid\", 270)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2 V2 - Trying regularization, larger model and more epochs"
      ],
      "metadata": {
        "id": "0XTllJjuCXZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "PXsp7r5jCkCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install miditok>=3.0.0 symusic transformers accelerate -q\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from miditok import REMI, TokenizerConfig\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f82jqK-NCaaZ",
        "outputId": "93845e48-109f-4816-c49e-e913d8fee0e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "DwZRrBFwCm8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = TokenizerConfig(\n",
        "    num_velocities=16,\n",
        "    use_chords=False,\n",
        "    use_programs=True,\n",
        "    one_token_stream_for_programs=True,\n",
        "    use_time_signatures=True,\n",
        "    use_tempos=True,\n",
        "    nb_tempos=32,\n",
        "    tempo_range=(100, 140),\n",
        "    beat_res={(0, 4): 8, (4, 12): 4},\n",
        ")\n",
        "\n",
        "tokenizer = REMI(config)\n",
        "print(f\"Vocab size: {len(tokenizer)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY8Tc_jGCoGO",
        "outputId": "41fd089d-6930-478c-f5c2-406ee3c4817d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3202952661.py:1: UserWarning: Argument nb_tempos has been renamed num_tempos, you should consider to updateyour code with this new argument name.\n",
            "  config = TokenizerConfig(\n",
            "/usr/local/lib/python3.12/dist-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
            "  super().__init__(tokenizer_config, params)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and tokenize MIDIs"
      ],
      "metadata": {
        "id": "eDTuxr6VCrGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "midi_files = list(Path('dance-midi').glob('*.mid')) + list(Path('dance-midi').glob('*.midi'))\n",
        "\n",
        "all_tokens = []\n",
        "for midi_path in midi_files:\n",
        "    try:\n",
        "        tokens = tokenizer(midi_path)\n",
        "        all_tokens.append(tokens.ids)\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {midi_path.name}: {e}\")\n",
        "\n",
        "total_tokens = sum(len(t) for t in all_tokens)\n",
        "print(f\"Tokenized {len(all_tokens)} files\")\n",
        "print(f\"Total tokens: {total_tokens:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE5_wWjsCshV",
        "outputId": "a1d35e38-8f25-42a4-ce28-26220ebc5ccc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized 38 files\n",
            "Total tokens: 1,041,915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset with more overlap"
      ],
      "metadata": {
        "id": "mn91jnApCxVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, all_tokens, seq_length=512, stride=256):  # 50% overlap\n",
        "        self.seq_length = seq_length\n",
        "        self.data = []\n",
        "        for tokens in all_tokens:\n",
        "            self.data.extend(tokens)\n",
        "        self.data = torch.tensor(self.data, dtype=torch.long)\n",
        "        self.indices = list(range(0, len(self.data) - seq_length - 1, stride))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = self.indices[idx]\n",
        "        chunk = self.data[start:start + self.seq_length + 1]\n",
        "        return {'input_ids': chunk[:-1], 'labels': chunk[1:]}\n",
        "\n",
        "dataset = MidiDataset(all_tokens, seq_length=512, stride=256)\n",
        "print(f\"Dataset samples: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJDpDuaLCw4m",
        "outputId": "c147e80d-1865-44e1-c0b4-976fab36607d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset samples: 4068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigger Model w/ Regularization (dropout)"
      ],
      "metadata": {
        "id": "4vLkvuXHDCJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = max(max(t) for t in all_tokens) + 1\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=vocab_size,\n",
        "    n_positions=512,\n",
        "    n_embd=512,       # Doubled from 256\n",
        "    n_layer=12,       # Doubled from 6\n",
        "    n_head=8,\n",
        "    resid_pdrop=0.1,  # Dropout on residual connections\n",
        "    embd_pdrop=0.1,   # Dropout on embeddings\n",
        "    attn_pdrop=0.1,   # Dropout on attention\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)\n",
        "model.to(device)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDz3dP_jDBpX",
        "outputId": "53a085e2-d8e9-4d60-b9ae-d286f3b904dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: 38,348,800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with early stopping"
      ],
      "metadata": {
        "id": "qhDM1YmUDZRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./midi-gpt2-large\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=30,          # Fewer epochs\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=1e-4,           # Lower learning rate\n",
        "    warmup_steps=200,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        "    weight_decay=0.1,             # L2 regularization\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EW0_pzRkDXSo",
        "outputId": "d358408f-db88-4ef0-af92-d01c6956bbef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3001' max='15270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3001/15270 09:20 < 38:12, 5.35 it/s, Epoch 5.89/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.938200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.383600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.866900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.420800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.313500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.201700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.152400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.058800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.979500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.958900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.906100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.872000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.880800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.820300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.806500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.774100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.748800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.719700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.684500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.601100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.562600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.503200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.482700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>1.381500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.359200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>1.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.232700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>1.176000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.200500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>1.108900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.086900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>1.056100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.052800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.999300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.022100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.956300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.914900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.889600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.860900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.886600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.865900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.853000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.817300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.807600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.814800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.776500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.730000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.737800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.726400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.711500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.680000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.672600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.676600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.683800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3140674665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2754\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2756\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2757\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3228\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3344\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3347\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3470\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3472\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3474\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1266\u001b[0m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try model at different checkpoints to check overfitting"
      ],
      "metadata": {
        "id": "2t02gRrMKiGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"./midi-gpt2-large/checkpoint-2000\")\n",
        "model = model.to('cpu')\n",
        "model.eval()\n",
        "# Use first 20 tokens from real data as seed\n",
        "seed = all_tokens[0][:20]\n",
        "print(\"Seed tokens:\")\n",
        "for i, tok_id in enumerate(seed):\n",
        "    print(f\"  {i}: {tokenizer[tok_id]}\")\n",
        "\n",
        "seed_tokens = torch.tensor([seed])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "generated_midi = tokenizer.decode(generated_ids)\n",
        "generated_midi.dump_midi(\"generated_seeded.mid\")\n",
        "\n",
        "from symusic import Score\n",
        "score = Score(\"generated_seeded.mid\")\n",
        "print(f\"Total notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_seeded.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "-w6GM95nKcUT",
        "outputId": "167df0e7-44e7-45ad-eb74-5c302febf9cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed tokens:\n",
            "  0: Bar_None\n",
            "  1: TimeSig_4/4\n",
            "  2: Position_0\n",
            "  3: Tempo_140.0\n",
            "  4: Bar_None\n",
            "  5: TimeSig_4/4\n",
            "  6: Bar_None\n",
            "  7: TimeSig_4/4\n",
            "  8: Position_16\n",
            "  9: Program_-1\n",
            "  10: PitchDrum_36\n",
            "  11: Velocity_79\n",
            "  12: Duration_1.0.8\n",
            "  13: Position_24\n",
            "  14: Program_-1\n",
            "  15: PitchDrum_38\n",
            "  16: Velocity_103\n",
            "  17: Duration_0.2.8\n",
            "  18: Program_-1\n",
            "  19: PitchDrum_36\n",
            "Total notes: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d350e292-01f0-4d2b-afc8-86c5d9572b03\", \"generated_seeded.mid\", 70)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "song_tokens = all_tokens[0]\n",
        "start_pos = len(song_tokens) // 2\n",
        "seed = song_tokens[start_pos:start_pos + 100]\n",
        "\n",
        "seed_tokens = torch.tensor([seed])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.5,          # Much lower - more predictable\n",
        "        top_k=20,                 # Only top 20 choices\n",
        "        top_p=0.85,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.3,   # Stronger repetition penalty\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "score = tokenizer.decode(generated_ids)\n",
        "score.dump_midi(\"tainted_love_seed.mid\")\n",
        "\n",
        "print(f\"Notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"tainted_love_seed.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xLzjG9BKK-Dr",
        "outputId": "a8615021-eff6-4651-d002-9feb0bf03d0a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notes: 22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_54e819d6-877c-4fce-9fe9-0be3bb50d502\", \"tainted_love_seed.mid\", 384)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate long unique sample\n",
        "def generate_long_unique(model, tokenizer, all_tokens, num_chunks=12, chunk_length=400, overlap=50):\n",
        "    \"\"\"Generate longer sequences, starting from random positions in different songs\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Start with a random seed from a random song\n",
        "    song_idx = random.randint(0, len(all_tokens) - 1)\n",
        "    start_pos = len(all_tokens[song_idx]) // 2\n",
        "    generated = list(all_tokens[song_idx][start_pos:start_pos + 50])\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        context = generated[-overlap:]\n",
        "        input_tokens = torch.tensor([context])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_tokens,\n",
        "                max_length=len(context) + chunk_length,\n",
        "                temperature=1.0,          # Higher = more creative/random\n",
        "                top_k=100,                # More choices\n",
        "                top_p=0.95,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.3,   # Discourage repetition\n",
        "                no_repeat_ngram_size=8,   # No repeated 8-token phrases\n",
        "                pad_token_id=tokenizer['PAD_None'],\n",
        "                eos_token_id=tokenizer['EOS_None'],\n",
        "            )\n",
        "\n",
        "        new_tokens = output[0].tolist()[len(context):]\n",
        "        generated.extend(new_tokens)\n",
        "\n",
        "        # Progress\n",
        "        score = tokenizer.decode(generated)\n",
        "        notes = sum(len(t.notes) for t in score.tracks)\n",
        "        print(f\"Chunk {i+1}/{num_chunks}: {notes} notes, {len(generated)} tokens\")\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Generate ~5000 tokens (roughly 30-60 seconds of music)\n",
        "generated_ids = generate_long_unique(model, tokenizer, all_tokens, num_chunks=12, chunk_length=400, overlap=50)\n",
        "\n",
        "# Save\n",
        "score = tokenizer.decode(generated_ids)\n",
        "score.dump_midi(\"generated_long_unique.mid\")\n",
        "\n",
        "total_notes = sum(len(t.notes) for t in score.tracks)\n",
        "drum_notes = sum(len(t.notes) for t in score.tracks if t.is_drum)\n",
        "duration_bars = score.end() / score.ticks_per_quarter / 4\n",
        "\n",
        "print(f\"\\n=== Final ===\")\n",
        "print(f\"Total notes: {total_notes} ({drum_notes} drums, {total_notes - drum_notes} other)\")\n",
        "print(f\"Duration: ~{duration_bars:.0f} bars\")\n",
        "print(f\"Tokens: {len(generated_ids)}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"generated_long_unique.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "qXVogtMQMZfR",
        "outputId": "a362fa48-7ca2-4f3e-caeb-56939e86214c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1/12: 11 notes, 450 tokens\n",
            "Chunk 2/12: 11 notes, 850 tokens\n",
            "Chunk 3/12: 11 notes, 1250 tokens\n",
            "Chunk 4/12: 11 notes, 1650 tokens\n",
            "Chunk 5/12: 11 notes, 2050 tokens\n",
            "Chunk 6/12: 11 notes, 2450 tokens\n",
            "Chunk 7/12: 11 notes, 2850 tokens\n",
            "Chunk 8/12: 11 notes, 3250 tokens\n",
            "Chunk 9/12: 11 notes, 3650 tokens\n",
            "Chunk 10/12: 11 notes, 4050 tokens\n",
            "Chunk 11/12: 11 notes, 4450 tokens\n",
            "Chunk 12/12: 11 notes, 4850 tokens\n",
            "\n",
            "=== Final ===\n",
            "Total notes: 11 (2 drums, 9 other)\n",
            "Duration: ~1 bars\n",
            "Tokens: 4850\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_92f5227e-9ada-41c9-81fb-98d325db560a\", \"generated_long_unique.mid\", 362)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Continuation Tests"
      ],
      "metadata": {
        "id": "BAZnWGQNNbhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: continue a real song and see if it sounds coherent\n",
        "song_idx = 1\n",
        "seed = all_tokens[song_idx][500:600]  # Middle of song\n",
        "\n",
        "seed_tokens = torch.tensor([seed])\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        seed_tokens,\n",
        "        max_length=500,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer['PAD_None'],\n",
        "        eos_token_id=tokenizer['EOS_None'],\n",
        "    )\n",
        "\n",
        "generated_ids = output[0].tolist()\n",
        "score = tokenizer.decode(generated_ids)\n",
        "score.dump_midi(\"continuation_test.mid\")\n",
        "\n",
        "print(f\"Notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"continuation_test.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "LbeWz2zrNGso",
        "outputId": "2c739e78-4a29-4cb7-c965-2c6f975017a6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notes: 21\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2428bf6e-daf4-4114-80f6-4f02d2415052\", \"continuation_test.mid\", 335)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def continue_song(model, tokenizer, seed_tokens, total_length=2000, chunk_length=400, overlap=100):\n",
        "    \"\"\"Continue from a seed, generating in chunks\"\"\"\n",
        "\n",
        "    generated = list(seed_tokens)\n",
        "\n",
        "    while len(generated) < total_length:\n",
        "        context = generated[-overlap:]\n",
        "        input_tokens = torch.tensor([context])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_tokens,\n",
        "                max_length=len(context) + chunk_length,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=tokenizer['PAD_None'],\n",
        "                eos_token_id=tokenizer['EOS_None'],\n",
        "            )\n",
        "\n",
        "        new_tokens = output[0].tolist()[len(context):]\n",
        "        generated.extend(new_tokens)\n",
        "\n",
        "        # Progress\n",
        "        score = tokenizer.decode(generated)\n",
        "        notes = sum(len(t.notes) for t in score.tracks)\n",
        "        bars = score.end() / score.ticks_per_quarter / 4\n",
        "        print(f\"Tokens: {len(generated)}, Notes: {notes}, Bars: {bars:.0f}\")\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Continue from middle of song 0\n",
        "song_idx = 0\n",
        "seed = all_tokens[song_idx][500:600]\n",
        "\n",
        "generated_ids = continue_song(model, tokenizer, seed, total_length=5000)\n",
        "\n",
        "score = tokenizer.decode(generated_ids)\n",
        "score.dump_midi(\"long_continuation.mid\")\n",
        "\n",
        "total_notes = sum(len(t.notes) for t in score.tracks)\n",
        "bars = score.end() / score.ticks_per_quarter / 4\n",
        "print(f\"\\n=== Final ===\")\n",
        "print(f\"Notes: {total_notes}\")\n",
        "print(f\"Bars: {bars:.0f}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"long_continuation.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "lCXmpVXxNhC1",
        "outputId": "602a2df2-bc28-49cf-f75c-5d8b4fc6cc75"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 500, Notes: 22, Bars: 2\n",
            "Tokens: 900, Notes: 22, Bars: 2\n",
            "Tokens: 1300, Notes: 22, Bars: 2\n",
            "Tokens: 1700, Notes: 22, Bars: 2\n",
            "Tokens: 2100, Notes: 22, Bars: 2\n",
            "Tokens: 2500, Notes: 22, Bars: 2\n",
            "Tokens: 2900, Notes: 22, Bars: 2\n",
            "Tokens: 3300, Notes: 22, Bars: 2\n",
            "Tokens: 3700, Notes: 22, Bars: 2\n",
            "Tokens: 4100, Notes: 22, Bars: 2\n",
            "Tokens: 4500, Notes: 22, Bars: 2\n",
            "Tokens: 4900, Notes: 22, Bars: 2\n",
            "Tokens: 5300, Notes: 22, Bars: 2\n",
            "\n",
            "=== Final ===\n",
            "Notes: 22\n",
            "Bars: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5087fd5a-e67c-45cf-8497-b86a6ac656b9\", \"long_continuation.mid\", 344)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def continue_song_debug(model, tokenizer, seed_tokens, total_length=3000, chunk_length=400, overlap=100):\n",
        "    \"\"\"Continue with debugging to see what's happening\"\"\"\n",
        "\n",
        "    generated = list(seed_tokens)\n",
        "\n",
        "    # Check initial seed\n",
        "    score = tokenizer.decode(generated)\n",
        "    initial_notes = sum(len(t.notes) for t in score.tracks)\n",
        "    print(f\"Seed: {len(generated)} tokens, {initial_notes} notes\")\n",
        "\n",
        "    chunk_num = 0\n",
        "    while len(generated) < total_length:\n",
        "        chunk_num += 1\n",
        "        context = generated[-overlap:]\n",
        "        input_tokens = torch.tensor([context])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_tokens,\n",
        "                max_length=len(context) + chunk_length,\n",
        "                temperature=0.75,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.2,\n",
        "                pad_token_id=tokenizer['PAD_None'],\n",
        "                eos_token_id=tokenizer['EOS_None'],\n",
        "            )\n",
        "\n",
        "        new_tokens = output[0].tolist()[len(context):]\n",
        "\n",
        "        # Debug: check what tokens are being generated\n",
        "        print(f\"\\nChunk {chunk_num}: {len(new_tokens)} new tokens\")\n",
        "        print(f\"First 20 new tokens:\")\n",
        "        for i, tok_id in enumerate(new_tokens[:20]):\n",
        "            print(f\"  {tokenizer[tok_id]}\")\n",
        "\n",
        "        generated.extend(new_tokens)\n",
        "\n",
        "        # Check if notes increased\n",
        "        score = tokenizer.decode(generated)\n",
        "        notes = sum(len(t.notes) for t in score.tracks)\n",
        "        bars = score.end() / score.ticks_per_quarter / 4\n",
        "        print(f\"Total: {len(generated)} tokens, {notes} notes, {bars:.0f} bars\")\n",
        "\n",
        "        if chunk_num >= 3:  # Just check first 3 chunks\n",
        "            break\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Debug\n",
        "seed = all_tokens[0][500:600]\n",
        "generated_ids = continue_song_debug(model, tokenizer, seed, total_length=3000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHmZfDWjORUv",
        "outputId": "d0a4d2a6-19da-4783-b368-6ebe1b3c9d65"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 100 tokens, 22 notes\n",
            "\n",
            "Chunk 1: 400 new tokens\n",
            "First 20 new tokens:\n",
            "  Bar_None\n",
            "  Position_0\n",
            "  Pitch_39\n",
            "  Duration_1.0.8\n",
            "  Pitch_75\n",
            "  Duration_1.0.8\n",
            "  Pitch_75\n",
            "  Duration_1.0.8\n",
            "  Program_32\n",
            "  Velocity_71\n",
            "  Program_16\n",
            "  Velocity_127\n",
            "  Program_17\n",
            "  Velocity_55\n",
            "  Program_53\n",
            "  Velocity_79\n",
            "  Position_8\n",
            "  Pitch_39\n",
            "  Duration_1.0.8\n",
            "  PitchDrum_38\n",
            "Total: 500 tokens, 22 notes, 2 bars\n",
            "\n",
            "Chunk 2: 400 new tokens\n",
            "First 20 new tokens:\n",
            "  Velocity_79\n",
            "  Position_24\n",
            "  PitchDrum_38\n",
            "  Duration_0.3.8\n",
            "  PitchDrum_36\n",
            "  Duration_1.0.8\n",
            "  TimeSig_4/4\n",
            "  Program_32\n",
            "  Velocity_71\n",
            "  Program_16\n",
            "  Velocity_127\n",
            "  Program_17\n",
            "  Velocity_55\n",
            "  Program_27\n",
            "  Velocity_111\n",
            "  Program_27\n",
            "  Velocity_79\n",
            "  Position_28\n",
            "  Pitch_55\n",
            "  Duration_0.4.8\n",
            "Total: 900 tokens, 22 notes, 2 bars\n",
            "\n",
            "Chunk 3: 400 new tokens\n",
            "First 20 new tokens:\n",
            "  Program_53\n",
            "  Velocity_79\n",
            "  Position_13\n",
            "  Pitch_63\n",
            "  Duration_0.5.8\n",
            "  PitchDrum_38\n",
            "  Duration_0.2.8\n",
            "  PitchDrum_36\n",
            "  Duration_1.0.8\n",
            "  Program_53\n",
            "  Velocity_103\n",
            "  Position_16\n",
            "  PitchDrum_36\n",
            "  Duration_1.0.8\n",
            "  Program_32\n",
            "  Velocity_71\n",
            "  Program_16\n",
            "  Velocity_127\n",
            "  Program_17\n",
            "  Velocity_47\n",
            "Total: 1300 tokens, 22 notes, 2 bars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare real vs generated token patterns\n",
        "print(\"=== REAL DATA (from training) ===\")\n",
        "real_tokens = all_tokens[0][500:550]\n",
        "for i, tok_id in enumerate(real_tokens):\n",
        "    print(f\"{i:3d}: {tokenizer[tok_id]}\")\n",
        "\n",
        "print(\"\\n=== GENERATED ===\")\n",
        "for i, tok_id in enumerate(generated_ids[100:150]):\n",
        "    print(f\"{i:3d}: {tokenizer[tok_id]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdmDOhc4OgcK",
        "outputId": "caded001-752c-49e4-9c67-c85e046e1e96"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== REAL DATA (from training) ===\n",
            "  0: Velocity_87\n",
            "  1: Duration_0.3.8\n",
            "  2: Position_24\n",
            "  3: Program_-1\n",
            "  4: PitchDrum_38\n",
            "  5: Velocity_103\n",
            "  6: Duration_0.3.8\n",
            "  7: Program_-1\n",
            "  8: PitchDrum_36\n",
            "  9: Velocity_79\n",
            " 10: Duration_1.0.8\n",
            " 11: Bar_None\n",
            " 12: TimeSig_4/4\n",
            " 13: Position_0\n",
            " 14: Program_32\n",
            " 15: Pitch_31\n",
            " 16: Velocity_71\n",
            " 17: Duration_0.5.8\n",
            " 18: Program_16\n",
            " 19: Pitch_31\n",
            " 20: Velocity_127\n",
            " 21: Duration_0.5.8\n",
            " 22: Program_61\n",
            " 23: Pitch_55\n",
            " 24: Velocity_127\n",
            " 25: Duration_0.5.8\n",
            " 26: Program_61\n",
            " 27: Pitch_43\n",
            " 28: Velocity_127\n",
            " 29: Duration_0.5.8\n",
            " 30: Program_17\n",
            " 31: Pitch_67\n",
            " 32: Velocity_63\n",
            " 33: Duration_0.5.8\n",
            " 34: Program_-1\n",
            " 35: PitchDrum_36\n",
            " 36: Velocity_79\n",
            " 37: Duration_1.0.8\n",
            " 38: Position_8\n",
            " 39: Program_32\n",
            " 40: Pitch_31\n",
            " 41: Velocity_71\n",
            " 42: Duration_1.1.8\n",
            " 43: Program_16\n",
            " 44: Pitch_31\n",
            " 45: Velocity_127\n",
            " 46: Duration_1.1.8\n",
            " 47: Program_61\n",
            " 48: Pitch_43\n",
            " 49: Velocity_111\n",
            "\n",
            "=== GENERATED ===\n",
            "  0: Bar_None\n",
            "  1: Position_0\n",
            "  2: Pitch_39\n",
            "  3: Duration_1.0.8\n",
            "  4: Pitch_75\n",
            "  5: Duration_1.0.8\n",
            "  6: Pitch_75\n",
            "  7: Duration_1.0.8\n",
            "  8: Program_32\n",
            "  9: Velocity_71\n",
            " 10: Program_16\n",
            " 11: Velocity_127\n",
            " 12: Program_17\n",
            " 13: Velocity_55\n",
            " 14: Program_53\n",
            " 15: Velocity_79\n",
            " 16: Position_8\n",
            " 17: Pitch_39\n",
            " 18: Duration_1.0.8\n",
            " 19: PitchDrum_38\n",
            " 20: Duration_0.2.8\n",
            " 21: PitchDrum_36\n",
            " 22: Duration_1.0.8\n",
            " 23: Program_32\n",
            " 24: Velocity_71\n",
            " 25: Program_16\n",
            " 26: Velocity_127\n",
            " 27: Program_17\n",
            " 28: Velocity_79\n",
            " 29: Position_16\n",
            " 30: Pitch_60\n",
            " 31: Duration_0.6.8\n",
            " 32: Pitch_36\n",
            " 33: Duration_1.4.8\n",
            " 34: Pitch_36\n",
            " 35: Duration_1.4.8\n",
            " 36: Pitch_72\n",
            " 37: Duration_1.4.8\n",
            " 38: Pitch_72\n",
            " 39: Duration_1.4.8\n",
            " 40: Pitch_64\n",
            " 41: Duration_1.4.8\n",
            " 42: Pitch_36\n",
            " 43: Duration_1.4.8\n",
            " 44: Program_-1\n",
            " 45: Velocity_103\n",
            " 46: Position_24\n",
            " 47: PitchDrum_38\n",
            " 48: Duration_0.2.8\n",
            " 49: PitchDrum_36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_constrained(model, tokenizer, seed, max_new_tokens=500):\n",
        "    \"\"\"Generate while enforcing valid token order\"\"\"\n",
        "\n",
        "    generated = list(seed)\n",
        "\n",
        "    # Track what token type we expect next\n",
        "    # Valid order: Position -> Program -> Pitch -> Velocity -> Duration\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        context = generated[-100:]\n",
        "        input_tokens = torch.tensor([context])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tokens)\n",
        "            logits = outputs.logits[0, -1, :]  # Last position\n",
        "\n",
        "        # Get last token to determine what should come next\n",
        "        last_tok = tokenizer[generated[-1]]\n",
        "\n",
        "        # Mask invalid tokens based on grammar\n",
        "        mask = torch.zeros_like(logits)\n",
        "\n",
        "        if 'Position' in last_tok or 'Bar' in last_tok or 'TimeSig' in last_tok:\n",
        "            # After Position/Bar, allow Program or Tempo\n",
        "            for i in range(len(tokenizer)):\n",
        "                tok = tokenizer[i]\n",
        "                if 'Program' in tok or 'Tempo' in tok:\n",
        "                    mask[i] = 1\n",
        "        elif 'Program' in last_tok:\n",
        "            # After Program, allow Pitch or PitchDrum\n",
        "            for i in range(len(tokenizer)):\n",
        "                tok = tokenizer[i]\n",
        "                if 'Pitch' in tok:\n",
        "                    mask[i] = 1\n",
        "        elif 'Pitch' in last_tok:\n",
        "            # After Pitch, allow Velocity\n",
        "            for i in range(len(tokenizer)):\n",
        "                tok = tokenizer[i]\n",
        "                if 'Velocity' in tok:\n",
        "                    mask[i] = 1\n",
        "        elif 'Velocity' in last_tok:\n",
        "            # After Velocity, allow Duration\n",
        "            for i in range(len(tokenizer)):\n",
        "                tok = tokenizer[i]\n",
        "                if 'Duration' in tok:\n",
        "                    mask[i] = 1\n",
        "        elif 'Duration' in last_tok:\n",
        "            # After Duration, allow Position, Bar, Program (next note same position)\n",
        "            for i in range(len(tokenizer)):\n",
        "                tok = tokenizer[i]\n",
        "                if 'Position' in tok or 'Bar' in tok or 'Program' in tok:\n",
        "                    mask[i] = 1\n",
        "        else:\n",
        "            mask = torch.ones_like(logits)  # Allow anything\n",
        "\n",
        "        # Apply mask\n",
        "        logits = logits + (mask - 1) * 10000\n",
        "\n",
        "        # Sample\n",
        "        probs = torch.softmax(logits / 0.8, dim=-1)\n",
        "        next_token = torch.multinomial(probs, 1).item()\n",
        "        generated.append(next_token)\n",
        "\n",
        "        if len(generated) % 100 == 0:\n",
        "            score = tokenizer.decode(generated)\n",
        "            notes = sum(len(t.notes) for t in score.tracks)\n",
        "            print(f\"Tokens: {len(generated)}, Notes: {notes}\")\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Test constrained generation\n",
        "seed = all_tokens[0][500:600]\n",
        "generated_ids = generate_constrained(model, tokenizer, seed, max_new_tokens=500)\n",
        "\n",
        "score = tokenizer.decode(generated_ids)\n",
        "score.dump_midi(\"constrained.mid\")\n",
        "print(f\"Total notes: {sum(len(t.notes) for t in score.tracks)}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"constrained.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "VTJJ7wJoOufJ",
        "outputId": "9ed73539-4ab0-425f-f801-030744171584"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 200, Notes: 47\n",
            "Tokens: 300, Notes: 72\n",
            "Tokens: 400, Notes: 97\n",
            "Tokens: 500, Notes: 122\n",
            "Tokens: 600, Notes: 147\n",
            "Total notes: 147\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_07f8708c-2fea-43be-946a-d839182d6f07\", \"constrained.mid\", 1749)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(midi_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8nLBLIrLxVs",
        "outputId": "881c5234-59fe-4ef2-f775-59baa2c6b9ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PosixPath('dance-midi/TaintedLove.mid'), PosixPath('dance-midi/SexyBitch.mid'), PosixPath('dance-midi/Rose_Royce_-_Car_Wash.mid'), PosixPath('dance-midi/YouSpinMeRound.mid'), PosixPath('dance-midi/Gloria_Gaynor_-_I_Will_Survive.mid'), PosixPath('dance-midi/People on the high line.mid'), PosixPath('dance-midi/KC_and_The_Sunshine_Band_-_KC_Medley.mid'), PosixPath('dance-midi/David_Bowie_-_Lets_Dance.mid'), PosixPath('dance-midi/Hot_Chocolate_-_You_Sexy_Thing.mid'), PosixPath('dance-midi/AroundTheWorld.mid'), PosixPath('dance-midi/New_Order_-_Blue_Monday.mid'), PosixPath('dance-midi/keep it comin love.mid'), PosixPath('dance-midi/Michael_Jackson_-_Thriller.mid'), PosixPath('dance-midi/Rick_James_-_Super_Freak.mid'), PosixPath('dance-midi/Kool_and_the_Gang_-_Ladies_Night.mid'), PosixPath('dance-midi/Frankie_Goes_to_Hollywood_-_Relax.mid'), PosixPath('dance-midi/KC_and_The_Sunshine_Band_-_Im_Your_Boogie_Man.mid'), PosixPath('dance-midi/Donna_Summer_-_I_Feel_Love.mid'), PosixPath('dance-midi/KC_and_The_Sunshine_Band_-_Thats_the_Way_I_Like_It.mid'), PosixPath('dance-midi/Kool_and_the_Gang_-_Jungle_Boogie.mid'), PosixPath('dance-midi/Lipps_Inc_-_Funkytown.mid'), PosixPath('dance-midi/RockDj.mid'), PosixPath('dance-midi/la bamba.mid'), PosixPath('dance-midi/Darude_-_Sandstorm.mid'), PosixPath('dance-midi/Kiki_Dee_-_Ive_Got_the_Music_in_Me.mid'), PosixPath('dance-midi/GetluckyfeatPharrellWilliams.mid'), PosixPath('dance-midi/Venga_Boys_-_Boom_Boom_Boom_Boom.mid'), PosixPath('dance-midi/DontYouWantMe.mid'), PosixPath('dance-midi/Mustang Sally.mid'), PosixPath('dance-midi/Im a believer.mid'), PosixPath('dance-midi/Alice_Deejay_-_Better_Off_Alone.mid'), PosixPath('dance-midi/Wild_Cherry_-_Play_That_Funky_Music.mid'), PosixPath('dance-midi/CantGetYououtofMyHead(3).mid'), PosixPath('dance-midi/NaturalBlues.mid'), PosixPath('dance-midi/Kool_and_the_Gang_-_Get_Down_On_It.mid'), PosixPath('dance-midi/C+C_Dance_Factory_-_Gonna_Make_You_Sweat.mid'), PosixPath('dance-midi/faith.mid'), PosixPath('dance-midi/fallinlove2nite.mid')]\n"
          ]
        }
      ]
    }
  ]
}